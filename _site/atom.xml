<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>David Shao's Zone</title>
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <link rel="stylesheet" href="/css/default.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile.css" type="text/css" />
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>
</head>
<body>
  <?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

   <title>David Shao's Zone</title>
   <link href="http://davidhhshao.github.io/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://davidhhshao.github.io" rel="alternate" type="text/html" />
   <updated>2015-07-10T15:41:04-07:00</updated>
   <id>http://davidhhshao.github.io</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>［touch spark］1. 准备和初次体验spark</title>
     <link href="/learning-spark-step-1"/>
     <updated>2014-11-30T00:00:00-08:00</updated>
     <id>/learning-spark-step-1</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 下载，安装，运行spark&lt;/h2&gt;

&lt;p&gt;　　下载最新版的&lt;a href=&quot;http://spark.apache.org/downloads.html&quot;&gt;spark&lt;/a&gt;. 我选择的是spark-1.1.1-bin-hadoop2.4.tgz。因为是预编译过的，所以没有所谓的“安装”环境，直接解压即可运行了。&lt;br/&gt;
　　运行路径是：spark-1.1.1-bin-hadoop2.4/bin，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chenshan@mac007 spark$ls
spark-1.1.1-bin-hadoop2.4.tgz
chenshan@mac007 spark$tar zxf spark-1.1.1-bin-hadoop2.4.tgz 
chenshan@mac007 spark$ls
chenshan@mac007 spark$cd spark-1.1.1-bin-hadoop2.4
chenshan@mac007 spark-1.1.1-bin-hadoop2.4$ls
CHANGES.txt NOTICE      RELEASE     conf        examples    python
LICENSE     README.md   bin         ec2         lib         sbin
chenshan@mac007 spark-1.1.1-bin-hadoop2.4$cd bin
chenshan@mac007 bin$ls
beeline               pyspark               run-example.cmd       spark-class2.cmd      spark-submit
compute-classpath.cmd pyspark.cmd           run-example2.cmd      spark-shell           spark-submit.cmd
compute-classpath.sh  pyspark2.cmd          spark-class           spark-shell.cmd       utils.sh
load-spark-env.sh     run-example           spark-class.cmd       spark-sql
chenshan@mac007 bin$./pyspark 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;2. 设置环境变量&lt;/h2&gt;

&lt;p&gt;　　为了方便以后使用，最好把bin文件夹加入环境变量，关于环境变量的设置可以参考&lt;a href=&quot;http://www.tuicool.com/articles/7nu2E3R&quot;&gt;这里&lt;/a&gt;。编辑～/.bash_profile，在最后加上一句：export PATH=~/Desktop/spark/spark-1.1.1-bin-hadoop2.4/bin:$PATH，保存文件后退出，在执行 source ～/.bash_profile 或 . ～/.bash_profile 即可生效。&lt;br/&gt;
　　ok，现在可以随时随地在任何路径下执行pyspark启动spark了。因为默认是用python的console来启动的，这样也可以用，但在python console里编码多多少少有很多不便，对我个人而言就是一些ls/cd啊这些常用命令，以及查看每个对象但方法和属性和自动补全了。所以这里如果可以在ipython里启动spark的话不就完美了。先google了下，没什么结果。那就来看看pyspark这个脚本里是怎么写的吧，果不其然，在最后几行还真找到东西了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  if [[ &quot;$IPYTHON&quot; = &quot;1&quot; ]]; then
    exec ipython $IPYTHON_OPTS
  else
    exec &quot;$PYSPARK_PYTHON&quot;
  fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　好，神秘钥匙找到了，看来我们需要设置一个IPYTHON的变量，且把该变量设置成1。ok，试一下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;chenshan@mac007 bin$$IPTYHON
chenshan@mac007 bin$export IPYTHON=&quot;1&quot;
chenshan@mac007 bin$$IPTYHON
-bash: 1: command not found
chenshan@mac007 bin$pyspark 
Python 2.7.5 (default, Mar  9 2014, 22:15:05) 
Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.

IPython 2.1.0 -- An enhanced Interactive Python.
?         -&amp;gt; Introduction and overview of IPython&#39;s features.
%quickref -&amp;gt; Quick reference.
help      -&amp;gt; Python&#39;s own help system.
object?   -&amp;gt; Details about &#39;object&#39;, use &#39;object??&#39; for extra details.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　成功，我想要的效果达到了，看下面。后来才发现原来官方上面已经有这个说明了，只是感觉也说得不是很明确，看&lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [1]: ls
README.md

In [2]: t = sc.textFile(&quot;R
README.md       ReferenceError  RuntimeError    RuntimeWarning  

In [2]: t = sc.textFile(&quot;README.md&quot;)
14/11/30 12:48:12 INFO MemoryStore: ensureFreeSpace(172851) called with curMem=0, maxMem=286300569
14/11/30 12:48:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 168.8 KB, free 272.9 MB)

In [3]: t.
t.aggregate                  t.foreachPartition           t.max                        t.setName
t.aggregateByKey             t.getCheckpointFile          t.mean                       t.sortBy
t.cache                      t.getNumPartitions           t.min                        t.sortByKey
t.cartesian                  t.getStorageLevel            t.name                       t.stats
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3. 单机版的spark应用&lt;/h2&gt;

&lt;p&gt;　　官方的&lt;a href=&quot;http://spark.apache.org/docs/latest/quick-start.html&quot;&gt;quick start&lt;/a&gt;里已经有了一个单机版的spark应用的示例。很简单的例子，但我觉得还是有一个地方需要理解下才行。那就是运行python写但spark应用但方法，需要使用spark安装目录里bin可执行文件目录下但spark-submit来运行。我第一次运行这个示例的时候，很不知所谓地就python SimpleApp.py，然后自然而然地提示我没有pyspark这个包，我心想，没有这个包还不好办吗，直接pip install pyspark，可pip居然提示也没有这个包。我想难道是pip抽风了，那好，我就google下pyspark，按理说第一项应该就是pyspark在pip的网页了，我靠，可连google也没搜出什么东西来啊。这下我觉得应该是我的问题了。想了下，先启动pyspark来看看有没有这个包，居然是有的，这？执行pyspark.&lt;strong&gt;file&lt;/strong&gt;来看看这个包的路径，我才恍然大悟。原来是我短路了，spark官方都说了，人家自己提供了scala，java，python都接口，全部都在源码里了，还提供了一些基于这三种语言都examples。只是这个包没有放到pip里去，而是随spark一起发布的。我想是因为spark现在还不够成熟，基于python的接口也会跟着每一个小版本的spark发布而改变，所以暂时没把pyspark发布到pip上去吧。&lt;br/&gt;
　　所以，这里值得注意的是，一定要用spark-submit来运行python写的spark应用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;In [13]: import pyspark

In [14]: pyspark.__file__
Out[14]: &#39;/Users/chenshan/Desktop/spark/spark-1.1.1-bin-hadoop2.4/python/pyspark/__init__.pyc&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　&lt;/p&gt;

&lt;h2&gt;Scan It!&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../images/share/2014-11-30-learning-spark-step-1.md.jpg&quot; alt=&quot;2014-11-30-learning-spark-step-1.md&quot; /&gt;&lt;/p&gt;
</content>
   </entry>
   

</feed>


</body>
</html>
